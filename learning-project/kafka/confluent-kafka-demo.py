from confluent_kafka import Producer, Consumer, KafkaError, KafkaException
import json
import time
import argparse
import socket

# Kafka é…ç½®
BOOTSTRAP_SERVERS = 'localhost:9092'  # é»˜è®¤æœ¬åœ°åœ°å€
TOPIC_NAME = 'fastapi-kafka'  # æµ‹è¯•ç”¨ä¸»é¢˜
GROUP_ID = 'test_group'  # æ¶ˆè´¹è€…ç»„ID


def test_kafka_connection():
    """æµ‹è¯•æ˜¯å¦èƒ½è¿æ¥åˆ° Kafka æœåŠ¡å™¨"""
    try:
        # åˆ›å»ºä¸´æ—¶ç”Ÿäº§è€…æµ‹è¯•è¿æ¥
        test_producer = Producer({
            'bootstrap.servers': BOOTSTRAP_SERVERS,
            'client.id': socket.gethostname(),
            'message.timeout.ms': 3000  # 3ç§’è¶…æ—¶
        })

        # å°è¯•åˆ·æ–°ç”Ÿäº§è€…ï¼ˆå¼ºåˆ¶è¿æ¥ï¼‰
        test_producer.flush(timeout=3.0)
        print("âœ… æˆåŠŸè¿æ¥åˆ° Kafka æœåŠ¡å™¨")
        return True
    except KafkaException as e:
        print(f"âŒ Kafka è¿æ¥é”™è¯¯: {e}")
        if e.args[0].code() == KafkaError._TIMED_OUT:
            print("   åŸå› : è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥:")
            print(f"    1. Kafka æ˜¯å¦åœ¨ {BOOTSTRAP_SERVERS} è¿è¡Œ?")
            print("    2. é˜²ç«å¢™æ˜¯å¦é˜»æ­¢äº†è¿æ¥?")
        return False
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {str(e)}")
        return False


def delivery_report(err, msg):
    """ç”Ÿäº§è€…æ¶ˆæ¯ä¼ é€’å›è°ƒå‡½æ•°"""
    if err is not None:
        print(f'âŒ æ¶ˆæ¯å‘é€å¤±è´¥: {err}')
    else:
        print(f"âœ… æ¶ˆæ¯å·²å‘é€: [åˆ†åŒº {msg.partition()}] {msg.value().decode('utf-8')}")


def produce_messages(num_messages=5):
    """ç”Ÿäº§è€…ï¼šå‘é€æµ‹è¯•æ¶ˆæ¯"""
    print(f"\nğŸ“¤ ç”Ÿäº§è€…å¯åŠ¨ï¼Œå°†å‘é€ {num_messages} æ¡æ¶ˆæ¯åˆ°ä¸»é¢˜ '{TOPIC_NAME}'...")

    # åˆ›å»ºç”Ÿäº§è€…é…ç½®
    conf = {
        'bootstrap.servers': BOOTSTRAP_SERVERS,
        'client.id': socket.gethostname(),
        'message.timeout.ms': 5000,
        'acks': 'all'  # ç¡®ä¿æ¶ˆæ¯è¢«å®Œå…¨æäº¤
    }

    try:
        producer = Producer(conf)

        # å‘é€æ¶ˆæ¯
        for i in range(num_messages):
            message = {
                "id": i,
                "text": f"Confluent æµ‹è¯•æ¶ˆæ¯ #{i}",
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            json_message = json.dumps(message)

            # å¼‚æ­¥å‘é€æ¶ˆæ¯ï¼ˆå¸¦å›è°ƒï¼‰
            producer.produce(
                TOPIC_NAME,
                key=str(i),
                value=json_message,
                callback=delivery_report
            )

            # è½®è¯¢äº‹ä»¶ï¼ˆè§¦å‘å›è°ƒï¼‰
            producer.poll(0)
            time.sleep(0.5)  # é¿å…å‘é€è¿‡å¿«

        # ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯å‘é€å®Œæˆ
        producer.flush()
        print("âœ… æ‰€æœ‰æ¶ˆæ¯å·²å‘é€æˆåŠŸï¼")
        return True

    except KafkaException as e:
        print(f"âŒ Kafka ç”Ÿäº§è€…é”™è¯¯: {e}")
        return False
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {str(e)}")
        return False


def consume_messages(timeout=10):
    """æ¶ˆè´¹è€…ï¼šæ¥æ”¶æ¶ˆæ¯"""
    print(f"\nğŸ“¥ æ¶ˆè´¹è€…å¯åŠ¨ï¼Œç›‘å¬ä¸»é¢˜ '{TOPIC_NAME}'ï¼Œç­‰å¾… {timeout} ç§’...")

    # åˆ›å»ºæ¶ˆè´¹è€…é…ç½®
    conf = {
        'bootstrap.servers': BOOTSTRAP_SERVERS,
        'group.id': GROUP_ID,
        'auto.offset.reset': 'earliest',  # ä»æœ€æ—©çš„æ¶ˆæ¯å¼€å§‹
        'enable.auto.commit': False,  # æ‰‹åŠ¨æäº¤åç§»é‡
        'max.poll.interval.ms': 600000  # å»¶é•¿è½®è¯¢é—´éš”
    }

    try:
        consumer = Consumer(conf)
        consumer.subscribe([TOPIC_NAME])

        start_time = time.time()
        message_count = 0

        # è½®è¯¢æ¶ˆæ¯
        while time.time() - start_time < timeout:
            msg = consumer.poll(timeout=1.0)  # 1ç§’è¶…æ—¶

            if msg is None:
                continue
            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    # åˆ†åŒºç»“æŸï¼ˆæ­£å¸¸æƒ…å†µï¼‰
                    continue
                else:
                    print(f"âŒ æ¶ˆè´¹è€…é”™è¯¯: {msg.error()}")
                    break

            # æˆåŠŸæ¥æ”¶æ¶ˆæ¯
            try:
                message = json.loads(msg.value().decode('utf-8'))
                print(f"  æ”¶åˆ°æ¶ˆæ¯: [åˆ†åŒº {msg.partition()}] {message['text']}")
                message_count += 1

                # æ‰‹åŠ¨æäº¤åç§»é‡ï¼ˆç¡®ä¿è‡³å°‘ä¸€æ¬¡å¤„ç†ï¼‰
                consumer.commit(asynchronous=False)
            except Exception as e:
                print(f"âŒ æ¶ˆæ¯è§£æé”™è¯¯: {str(e)}")

        print(f"âœ… å…±æ”¶åˆ° {message_count} æ¡æ¶ˆæ¯")
        return True

    except KafkaException as e:
        print(f"âŒ Kafka æ¶ˆè´¹è€…é”™è¯¯: {e}")
        return False
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {str(e)}")
        return False
    finally:
        try:
            consumer.close()
        except:
            pass


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Confluent Kafka æµ‹è¯•å·¥å…·')
    parser.add_argument('--test', action='store_true', help='ä»…æµ‹è¯•è¿æ¥')
    parser.add_argument('--produce', action='store_true', help='è¿è¡Œç”Ÿäº§è€…')
    parser.add_argument('--consume', action='store_true', help='è¿è¡Œæ¶ˆè´¹è€…')
    parser.add_argument('--full', action='store_true', help='å®Œæ•´æµ‹è¯•ï¼ˆè¿æ¥+ç”Ÿäº§+æ¶ˆè´¹ï¼‰')

    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if not any(vars(args).values()):
        parser.print_help()
        exit(1)

    # æ£€æŸ¥è¿æ¥
    if not test_kafka_connection():
        exit(1)

    if args.test:
        exit(0)

    # æ‰§è¡Œå®Œæ•´æµ‹è¯•
    if args.full:
        if produce_messages():
            time.sleep(1)  # ç»™ Kafka ä¸€äº›å¤„ç†æ—¶é—´
            consume_messages()
        exit(0)

    # å•ç‹¬æ‰§è¡Œç”Ÿäº§è€…
    if args.produce:
        produce_messages()

    # å•ç‹¬æ‰§è¡Œæ¶ˆè´¹è€…
    if args.consume:
        consume_messages()